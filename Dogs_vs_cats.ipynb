{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''\n",
    "IMG_SIZE = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(kind):\n",
    "    data = []\n",
    "    main_dir = os.path.join(path, kind)\n",
    "    cats_dir = os.path.join(main_dir, 'cats')\n",
    "    dogs_dir = os.path.join(main_dir, 'dogs')\n",
    "    for img in tqdm(os.listdir(cats_dir)):\n",
    "            pathc = os.path.join(cats_dir, img)\n",
    "            img = cv2.imread(pathc, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE)) / 255.0\n",
    "            data.append([np.array(img), 0])\n",
    "    for img in tqdm(os.listdir(dogs_dir)):\n",
    "            pathd = os.path.join(dogs_dir, img)\n",
    "            img = cv2.imread(pathd, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))/ 255.0\n",
    "            data.append([np.array(img), 1])\n",
    "    shuffle(data)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11500/11500 [07:03<00:00, 27.15it/s]\n",
      "100%|██████████| 11500/11500 [06:37<00:00, 28.94it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 67.95it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 79.65it/s]\n"
     ]
    }
   ],
   "source": [
    "whole_data = create_data('train')\n",
    "valid_data = create_data('valid')\n",
    "training_data = whole_data[:18000]\n",
    "test_data = whole_data[18000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "for feature, label in training_data:\n",
    "    X_train.append(feature)\n",
    "    y_train.append(label)\n",
    "for feature, label in valid_data:\n",
    "    X_val.append(feature)\n",
    "    y_val.append(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import utils\n",
    "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train = np.array(y_train)\n",
    "y_train = utils.to_categorical(y_train, 2)\n",
    "X_val = np.array(X_val).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_val = np.array(y_val)\n",
    "y_val = utils.to_categorical(y_val, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(X_train.shape[1:])),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
    "    Conv2D(32, (5, 5), padding='same', activation='relu'),\n",
    "    Conv2D(64, (7, 7), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
    "    Conv2D(128, (9, 9), padding='same', activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.25),\n",
    "    Dense(512, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.0001), bias_regularizer=regularizers.l2(0.0001)),\n",
    "    Dense(2, activation='softmax')\n",
    "   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/9\n",
      "18000/18000 [==============================] - 132s 7ms/sample - loss: 0.6907 - accuracy: 0.6294 - val_loss: 0.6733 - val_accuracy: 0.6395\n",
      "Epoch 2/9\n",
      "18000/18000 [==============================] - 90s 5ms/sample - loss: 0.5446 - accuracy: 0.7529 - val_loss: 0.5379 - val_accuracy: 0.7530\n",
      "Epoch 3/9\n",
      "18000/18000 [==============================] - 89s 5ms/sample - loss: 0.4728 - accuracy: 0.7938 - val_loss: 0.4725 - val_accuracy: 0.7815\n",
      "Epoch 4/9\n",
      "18000/18000 [==============================] - 90s 5ms/sample - loss: 0.4331 - accuracy: 0.8155 - val_loss: 0.4562 - val_accuracy: 0.7920\n",
      "Epoch 5/9\n",
      "18000/18000 [==============================] - 90s 5ms/sample - loss: 0.3928 - accuracy: 0.8324 - val_loss: 0.4233 - val_accuracy: 0.8105\n",
      "Epoch 6/9\n",
      "18000/18000 [==============================] - 84s 5ms/sample - loss: 0.3528 - accuracy: 0.8566 - val_loss: 0.4604 - val_accuracy: 0.7880\n",
      "Epoch 7/9\n",
      "18000/18000 [==============================] - 78s 4ms/sample - loss: 0.3182 - accuracy: 0.8766 - val_loss: 0.3831 - val_accuracy: 0.8340\n",
      "Epoch 8/9\n",
      "18000/18000 [==============================] - 78s 4ms/sample - loss: 0.2812 - accuracy: 0.8934 - val_loss: 0.3697 - val_accuracy: 0.8415\n",
      "Epoch 9/9\n",
      "18000/18000 [==============================] - 79s 4ms/sample - loss: 0.2531 - accuracy: 0.9067 - val_loss: 0.3758 - val_accuracy: 0.8390\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=9, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for feature, label in test_data:\n",
    "    X_test.append(feature)\n",
    "    y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = np.array(y_test)\n",
    "y_test = utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 6s 1ms/sample - loss: 0.3750 - accuracy: 0.8440\n",
      "test loss, test acc: [0.3749985071182251, 0.844]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=64)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
